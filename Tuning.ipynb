{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "406e4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "626a69e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load datasets\n",
    "\n",
    "airbnb_crime = pd.read_csv('cleaned_airbnb_crime.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1a9b4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "--- Train Set ---\n",
      "R² Score: 0.5066141339246377\n",
      "RMSE: 143.0996877524965\n",
      "MAE: 31.600302682195714\n",
      "\n",
      "--- Test Set ---\n",
      "R² Score: 0.5396764129293494\n",
      "RMSE: 109.5243820851694\n",
      "MAE: 37.295600249624336\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Target Encoding Function\n",
    "# -----------------------------\n",
    "def target_encode(train_series, target_series, smoothing=10):\n",
    "    global_mean = target_series.mean()\n",
    "    agg = train_series.to_frame().join(target_series).groupby(train_series.name)[target_series.name].agg(['mean','count'])\n",
    "    smooth = (agg['count'] * agg['mean'] + smoothing * global_mean) / (agg['count'] + smoothing)\n",
    "    return train_series.map(smooth), smooth, global_mean\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Preprocessing\n",
    "# -----------------------------\n",
    "# Log-transform target\n",
    "y = np.log1p(airbnb_crime['price'])\n",
    "X = airbnb_crime.drop(columns=['price'])\n",
    "\n",
    "# Convert date columns to numeric (days since a reference date)\n",
    "if 'last_review' in X.columns:\n",
    "    X['last_review'] = pd.to_datetime(X['last_review'])\n",
    "    reference_date = X['last_review'].max()\n",
    "    X['days_since_last_review'] = (reference_date - X['last_review']).dt.days\n",
    "    X = X.drop(columns=['last_review'])\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Target encode categorical columns\n",
    "categorical_cols = ['neighbourhood_group_cleansed', 'neighbourhood_cleansed', 'room_type']  # add more if needed\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in X_train.columns:\n",
    "        train_encoded, encoding_map, global_mean = target_encode(X_train[col], y_train)\n",
    "        X_train[col + '_enc'] = train_encoded\n",
    "        X_test[col + '_enc'] = X_test[col].map(encoding_map).fillna(global_mean)\n",
    "\n",
    "# Drop original categorical columns\n",
    "X_train = X_train.drop(columns=[c for c in categorical_cols if c in X_train.columns])\n",
    "X_test = X_test.drop(columns=[c for c in categorical_cols if c in X_test.columns])\n",
    "\n",
    "# Keep only numeric columns\n",
    "X_train = X_train.select_dtypes(include=[np.number])\n",
    "X_test = X_test.select_dtypes(include=[np.number])\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Optional: Interaction Features\n",
    "# -----------------------------\n",
    "if 'neighbourhood_group_cleansed_enc' in X_train.columns and 'room_type_enc' in X_train.columns:\n",
    "    X_train['neigh_room_interaction'] = X_train['neighbourhood_group_cleansed_enc'] * X_train['room_type_enc']\n",
    "    X_test['neigh_room_interaction'] = X_test['neighbourhood_group_cleansed_enc'] * X_test['room_type_enc']\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ XGBoost Regressor with Hyperparameter Tuning\n",
    "# -----------------------------\n",
    "xgb = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [200, 400, 600],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=25,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Evaluate Model\n",
    "# -----------------------------\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    y_pred_train = np.expm1(model.predict(X_train))  # inverse log-transform\n",
    "    y_pred_test = np.expm1(model.predict(X_test))\n",
    "    \n",
    "    y_train_actual = np.expm1(y_train)\n",
    "    y_test_actual = np.expm1(y_test)\n",
    "    \n",
    "    print('--- Train Set ---')\n",
    "    print('R² Score:', r2_score(y_train_actual, y_pred_train))\n",
    "    print('RMSE:', np.sqrt(mean_squared_error(y_train_actual, y_pred_train)))\n",
    "    print('MAE:', mean_absolute_error(y_train_actual, y_pred_train))\n",
    "    \n",
    "    print('\\n--- Test Set ---')\n",
    "    print('R² Score:', r2_score(y_test_actual, y_pred_test))\n",
    "    print('RMSE:', np.sqrt(mean_squared_error(y_test_actual, y_pred_test)))\n",
    "    print('MAE:', mean_absolute_error(y_test_actual, y_pred_test))\n",
    "\n",
    "evaluate_model(best_model, X_train, y_train, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a3cd5a",
   "metadata": {},
   "source": [
    "1️⃣ Comparison of metrics\n",
    "Metric\tBefore\tAfter\n",
    "Train R²\t0.551\t0.507\n",
    "Train RMSE\t136.58\t143.10\n",
    "Train MAE\t31.62\t31.60\n",
    "Test R²\t0.572\t0.540\n",
    "Test RMSE\t105.64\t109.52\n",
    "Test MAE\t37.96\t37.30\n",
    "2️⃣ Observations\n",
    "\n",
    "Slight drop in R² and increase in RMSE\n",
    "\n",
    "After the full preprocessing + hyperparameter search, the train R² decreased slightly (0.551 → 0.507) and RMSE increased.\n",
    "\n",
    "Test R² also dropped (0.572 → 0.540) and RMSE slightly increased.\n",
    "\n",
    "This suggests the model may be slightly underfitting compared to the simpler baseline.\n",
    "\n",
    "MAE is roughly unchanged\n",
    "\n",
    "MAE stayed almost the same, indicating the median error magnitude hasn’t worsened — the model still predicts the typical price reasonably well.\n",
    "\n",
    "Regularization / hyperparameter tuning effect\n",
    "\n",
    "The RandomizedSearchCV might have picked more conservative hyperparameters, leading to slightly lower variance (less overfitting) but slightly higher bias.\n",
    "\n",
    "Sometimes the “improved” pipeline trades small accuracy for more robustness.\n",
    "\n",
    "Target encoding + interaction features\n",
    "\n",
    "These changes don’t always guarantee higher R² immediately — the effect depends on correlations in your dataset.\n",
    "\n",
    "Target encoding helps high-cardinality features, but if the feature doesn’t strongly predict the target, it can slightly increase noise.\n",
    "\n",
    "3️⃣ Recommendations to push performance further\n",
    "\n",
    "Feature selection / engineering\n",
    "\n",
    "Try removing weak features like days_since_last_review if they don’t correlate with price.\n",
    "\n",
    "Add more interaction terms or aggregated statistics (e.g., avg price per neighborhood).\n",
    "\n",
    "Try tree-based encoders\n",
    "\n",
    "Instead of simple target encoding, you could use CatBoost encoding or Leave-One-Out encoding to reduce leakage.\n",
    "\n",
    "Hyperparameter tuning adjustments\n",
    "\n",
    "Expand search space (e.g., higher max_depth or more n_estimators) to capture more complexity.\n",
    "\n",
    "Consider early stopping with eval_set to avoid underfitting.\n",
    "\n",
    "Outlier treatment\n",
    "\n",
    "Even with log-transform, extreme Airbnb prices can skew RMSE.\n",
    "\n",
    "Clipping or filtering extreme listings could slightly improve R².\n",
    "\n",
    "✅ Summary\n",
    "\n",
    "The “improved” pipeline is more robust but slightly underfits.\n",
    "\n",
    "MAE staying similar means typical predictions are stable.\n",
    "\n",
    "Further gains likely require additional feature engineering, careful hyperparameter tuning, and possibly outlier handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56382793",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
